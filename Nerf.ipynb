{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports needed to load the data, train the model, and plot its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "import cv2\n",
    "import imageio\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data to train the network: Code was adapted from the official NeRF repository to work with PyTorch. https://github.com/bmild/nerf/blob/master/load_blender.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translation matrix but 't' is only considered to be on the z axis. It basically translates the given point by t in the z axis direction.\n",
    "# Note : This is weird as in spherical coordinates for which these matrices are used, the given 't' value is usually the radius.\n",
    "trans_t = lambda t : torch.tensor([\n",
    "    [1,0,0,0],\n",
    "    [0,1,0,0],\n",
    "    [0,0,1,t],\n",
    "    [0,0,0,1]]).float()\n",
    "\n",
    "\n",
    "# Rx rotation matrix rotation. Rotation around the x axis. Angle given by 'phi'\n",
    "rot_phi = lambda phi : torch.tensor([\n",
    "    [1,0,0,0],\n",
    "    [0,np.cos(phi),-np.sin(phi),0],\n",
    "    [0,np.sin(phi), np.cos(phi),0],\n",
    "    [0,0,0,1]]).float()\n",
    "\n",
    "# Rz rotation matrix. Rotation around the z axis. Angle given by 'th'\n",
    "rot_theta = lambda th : torch.tensor([\n",
    "    [np.cos(th),0,-np.sin(th),0],\n",
    "    [0,1,0,0],\n",
    "    [np.sin(th),0, np.cos(th),0],\n",
    "    [0,0,0,1]]).float()\n",
    "\n",
    "\n",
    "def pose_spherical(theta, phi, radius):\n",
    "    '''\n",
    "        not sure about this :- seems to perform some sort of position transformation from a 'camera' reference frame to a general 'world' reference frame in carthesian cooridnates\n",
    "    '''\n",
    "    # translate point along the z axis by 'radius'\n",
    "    c2w = trans_t(radius)\n",
    "\n",
    "    # rotate point by phi around x axis\n",
    "    c2w = rot_phi(phi/180.*np.pi) @ c2w\n",
    "\n",
    "    # rotate point by theta around z(?) axis\n",
    "    c2w = rot_theta((theta/180.*np.pi)) @ c2w\n",
    "\n",
    "    # I don't understand this transform. It looks like its scaling the x coordinates by -1\n",
    "    c2w = torch.Tensor(np.array([[-1,0,0,0],\n",
    "                                 [ 0,0,1,0],\n",
    "                                 [ 0,1,0,0],\n",
    "                                 [ 0,0,0,1]])) @ c2w # Had to call double() on both tensors or the matmul() wouldn't work for some reason\n",
    "    return c2w\n",
    "    \n",
    "\n",
    "def load_blender_data(basedir, half_res=False, testskip=1):\n",
    "    '''\n",
    "        inputs : \n",
    "                basedir : (str) containing the base directory where the data can be found\n",
    "                half_res: (bool) reduces the resolution of the images to haf of its pixels if true\n",
    "                testskip: (int) sep with which images are loaded: 1 => all loaded, 2 => only half of them ...\n",
    "        outputs:\n",
    "                imgs: numpy array of images as array of RGB values por each pixel \n",
    "                not sure about this :- poses: numpy array of 4x4 transformation matrices giving the position and angle of the object with respect to a general reference frame : more info on how they work on https://www.brainvoyager.com/bv/doc/UsersGuide/CoordsAndTransforms/SpatialTransformationMatrices.html\n",
    "                render_poses: numpy array of 4x4 transformation matrices giving the position of the camera relative to a general reference frame in a circular path for rendering purposes.\n",
    "                [H, W, focal] : height of screen on which object is projected (pixels), width of screen on which object is proejected (pixels), focal distance : distance between camera and center of screen in some arbitrary unit\n",
    "                i_split :array of 3 arrays with the numbers of the indices of [ train , val, test] images\n",
    "    '''\n",
    "    splits = ['train', 'val', 'test']\n",
    "    metas = {}\n",
    "    for s in splits:\n",
    "        with open(os.path.join(basedir, 'transforms_{}.json'.format(s)), 'r') as fp:\n",
    "            metas[s] = json.load(fp)\n",
    "\n",
    "    all_imgs = []\n",
    "    all_poses = []\n",
    "    counts = [0]\n",
    "    for s in splits:\n",
    "        meta = metas[s]\n",
    "        imgs = []\n",
    "        poses = []\n",
    "        if s=='train' or testskip==0:\n",
    "            skip = 1\n",
    "        else:\n",
    "            skip = testskip\n",
    "            \n",
    "        for frame in meta['frames'][::skip]:\n",
    "            fname = os.path.join(basedir, frame['file_path'] + '.png')\n",
    "            imgs.append(imageio.imread(fname))\n",
    "            poses.append(np.array(frame['transform_matrix']))\n",
    "        imgs = (np.array(imgs) / 255.).astype(np.float32) # keep all 4 channels (RGBA)\n",
    "        poses = np.array(poses).astype(np.float32)\n",
    "        counts.append(counts[-1] + imgs.shape[0])\n",
    "        all_imgs.append(imgs)\n",
    "        all_poses.append(poses)\n",
    "    \n",
    "    i_split = [np.arange(counts[i], counts[i+1]) for i in range(3)]\n",
    "    \n",
    "    imgs = np.concatenate(all_imgs, 0)\n",
    "    poses = np.concatenate(all_poses, 0)\n",
    "    \n",
    "    H, W = imgs[0].shape[:2]\n",
    "    camera_angle_x = float(meta['camera_angle_x'])\n",
    "    focal = .5 * W / np.tan(.5 * camera_angle_x)\n",
    "    \n",
    "    render_poses = torch.stack([pose_spherical(angle, -30.0, 4.0) for angle in np.linspace(-180,180,40+1)[:-1]], 0)\n",
    "    \n",
    "    if half_res:\n",
    "        H = H//2\n",
    "        W = W//2\n",
    "        focal = focal/2.\n",
    "\n",
    "        imgs_half_res = np.zeros((imgs.shape[0], H, W, 4))\n",
    "        for i, img in enumerate(imgs):\n",
    "            imgs_half_res[i] = cv2.resize(img, (W, H), interpolation=cv2.INTER_AREA)\n",
    "        imgs = imgs_half_res\n",
    "        # imgs = tf.image.resize_area(imgs, [400, 400]).numpy()\n",
    "\n",
    "        \n",
    "    return imgs, poses, render_poses, [H, W, focal], i_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that will return the ray direction and origin for the images given in the dataset. Code adapted from the original NeRF repository : https://github.com/bmild/nerf/blob/master/run_nerf_helpers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_rays(H, W, focal, pose):\n",
    "    \"\"\"Get ray origins, and directions from a pinhole camera. given the 'pose' transform matrix to transform the direction and position \n",
    "       from standard camera at origin to actual position and direction in world cooridnates\"\"\"\n",
    "    i, j = torch.meshgrid(torch.linspace(0, W-1, W), torch.linspace(0, H-1, H), indexing='xy')\n",
    "    dirs = torch.stack([(i-W*.5)/focal, -(j-H*.5)/focal, -torch.ones_like(i)], -1)\n",
    "    rays_d =  torch.sum((dirs[..., np.newaxis, :]) * pose[:3,:3], -1)\n",
    "    rays_o = torch.tensor(pose[:3,-1]).expand(rays_d.shape)\n",
    "    return rays_o, rays_d\n",
    "\n",
    "def get_random_ray_batch(H,W, focal, pose, x_pixel_coord,y_pixel_coord):\n",
    "   \"get randomly nb_rays from the rays tha go through each pixel.\"\n",
    "   rays_o,rays_d = get_all_rays(H,W,focal,pose)\n",
    "\n",
    "   \n",
    "   return torch.squeeze(rays_o[x_pixel_coord,y_pixel_coord]), torch.squeeze(rays_d[x_pixel_coord,y_pixel_coord])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the NeRF neural network as defined in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeRF(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        input_position = 60\n",
    "        input_direction = 24\n",
    "        output_colour = 3\n",
    "        hidden_features = 256\n",
    "\n",
    "        self.l1 = nn.Linear(input_position,  hidden_features)\n",
    "        self.l2 = nn.Linear(hidden_features, hidden_features)\n",
    "        self.l3 = nn.Linear(hidden_features, hidden_features)\n",
    "        self.l4 = nn.Linear(hidden_features, hidden_features)\n",
    "        self.l5 = nn.Linear(hidden_features, hidden_features)\n",
    "        self.l6 = nn.Linear(hidden_features + input_position, hidden_features)\n",
    "        self.l7 = nn.Linear(hidden_features, hidden_features)\n",
    "        self.l8 = nn.Linear(hidden_features, hidden_features)        \n",
    "        self.l9 = nn.Linear(hidden_features, hidden_features)\n",
    "        self.l10 = nn.Linear(hidden_features+input_direction, 128)\n",
    "        self.l11 = nn.Linear(128, output_colour)\n",
    "\n",
    "        self.activationReLU = nn.ReLU()\n",
    "        self.activationSigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, pos, dir):\n",
    "\n",
    "        h1 = self.l1(self.activationReLU(pos))\n",
    "        h2 = self.l2(self.activationReLU(h1))\n",
    "        h3 = self.l3(self.activationReLU(h2))\n",
    "        h4 = self.l4(self.activationReLU(h3))\n",
    "        h5 = torch.cat((self.l5(self.activationReLU(h4)),pos))\n",
    "        h6 = self.l6(self.activationReLU(h5))\n",
    "        h7 = self.l7(self.activationReLU(h6))\n",
    "        h8 = self.l8(self.activationReLU(h7)) \n",
    "        h9 = self.l9(h8)# no activation function before layer 9\n",
    "        density = h9[0] #output density\n",
    "        h9 = torch.cat((h9,dir))#### attention possible bug : cat sur la bonne dimension\n",
    "        h10 = self.l10(self.activationReLU(h9)) \n",
    "        colour = self.l11(self.activationSigmoid(h10))\n",
    "\n",
    "        return density, colour\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fct(rgb_pred_coarse,rgb_pred_fine,rgb_true):\n",
    "    loss = 0\n",
    "    for i in range(len(rgb_pred_coarse)):\n",
    "        loss += (torch.norm(torch.sub(rgb_pred_coarse[i], rgb_true[i]),2) + torch.norm(torch.sub(rgb_pred_fine[i], rgb_true[i]),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the encoding function that will take the inputs of the neural network and project them to a higher dimension input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_fct(value,level):\n",
    "\n",
    "    encoded = torch.zeros(level*2)\n",
    "\n",
    "    for i in range(0,level*2, 2):\n",
    "        encoded[i] = torch.sin(torch.tensor(np.power(2,i)*np.pi*value))\n",
    "        encoded[i+1] = torch.cos(torch.tensor(np.power(2,i)*np.pi*value))\n",
    "        \n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query fct to get a sample from NeRF networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_from_NeRF(network,pos,direction):\n",
    "\n",
    "    l_pos = 10\n",
    "    l_dir = 4\n",
    "\n",
    "    pos_query = torch.empty(0)\n",
    "    dir_query = torch.empty(0)\n",
    "    \n",
    "    for coord in pos:\n",
    "        pos_query = torch.cat((pos_query, encoding_fct(coord, l_pos)))\n",
    "    \n",
    "    for d in direction:\n",
    "        dir_query = torch.cat((dir_query, encoding_fct(d,l_dir)))\n",
    "\n",
    "    density, colour = network.forward(pos_query,dir_query)\n",
    "    return density , colour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function the returns random samples following a probability distribution function computed from the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_sampling(weights,min_dist,distance_step,nb_samples_fine,nb_samples_coarse):\n",
    "    nb_bins = nb_samples_coarse + 2\n",
    "    cumulative_probability = torch.zeros(nb_bins)\n",
    "    bound_distance = torch.zeros(nb_bins)\n",
    "    bound_distance[0] = min_dist\n",
    "\n",
    "    for i in range(1,nb_bins-1):\n",
    "        cumulative_probability[i] = cumulative_probability[i-1] + weights[i-1]\n",
    "        bound_distance[i] = bound_distance[i-1] + distance_step\n",
    "\n",
    "    samples = torch.zeros(nb_samples_fine)\n",
    "\n",
    "    #generate as many numbers in the [0,1[ range as there are samples\n",
    "    random_samples = torch.rand(nb_samples_fine)\n",
    "\n",
    "    for i, rand_nb in enumerate(random_samples):\n",
    "        # for each random sample check within each bin of the cumulative pdf it is \n",
    "        for j in range(nb_bins - 1 ):\n",
    "            \n",
    "            if(rand_nb < cumulative_probability[j+1] and rand_nb >= cumulative_probability[j]):\n",
    "                # when in appropriate bin, redraw uniformly between lowest and largest distance of the bin\n",
    "                samples[i] = (bound_distance[j] - bound_distance[j+1]) * torch.rand(1) + bound_distance[j+1]\n",
    "                break\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical volume sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_volume_sampling(nb_coarse_samples,nb_fine_samples,coarse_net,fine_net,origin:torch.TensorType, direction:torch.TensorType,min_dist=0.,max_dist = 1.):\n",
    "    \n",
    "    coarse_samples_density = torch.zeros(nb_coarse_samples)\n",
    "    weights = torch.zeros(nb_coarse_samples)\n",
    "    weights = torch.zeros(nb_coarse_samples)\n",
    "    distance_fraction = torch.tensor(min_dist + max_dist) / (nb_coarse_samples + 1)\n",
    "    \n",
    "    coarse_samples_distance = torch.zeros(nb_coarse_samples)\n",
    "    coarse_colour_of_ray = torch.zeros(3)\n",
    "\n",
    "    for i in range(nb_coarse_samples):\n",
    "        \n",
    "        # equidistant samples with sam direction\n",
    "        coarse_samples_distance[i] = torch.add(min_dist, (torch.mul(distance_fraction,i + 1))) #sorted by definition\n",
    "\n",
    "        # we only care for the density of the coarse samples\n",
    "        coarse_samples_density[i],coarse_colour = query_from_NeRF(coarse_net, origin +  (direction * coarse_samples_distance[i]),direction)\n",
    "\n",
    "        #compute Ti value\n",
    "        Ti = torch.tensor(0)\n",
    "        for k in range(i):\n",
    "            #TODO : this should be simplified with exponent rules to not have to do a loop\n",
    "            Ti = torch.add(Ti,coarse_samples_density[k]*distance_fraction)\n",
    "        Ti = torch.exp(-Ti)\n",
    "        \n",
    "        #compute weights of importance of sample to generate pdf later\n",
    "        weights[i] = Ti * (1 - torch.exp(-distance_fraction * coarse_samples_density[i]))\n",
    "        coarse_colour_of_ray = weights[i] * coarse_colour\n",
    "    \n",
    "    # normalize\n",
    "    total_weights = torch.sum(weights)\n",
    "    weights = torch.div(weights,total_weights)\n",
    "    \n",
    "    # generate sample locations from pdf made with the weights:\n",
    "    fine_samples_distance = pdf_sampling(weights,min_dist,distance_fraction,nb_fine_samples,nb_coarse_samples)\n",
    "\n",
    "    #add the coarse locations to the set of lecations and sort them in incresing order\n",
    "    fine_samples_distance = torch.cat((fine_samples_distance,coarse_samples_distance))\n",
    "    fine_samples_distance,_ = torch.sort(fine_samples_distance)\n",
    "    \n",
    "    fine_samples_density = torch.zeros((nb_coarse_samples + nb_fine_samples))\n",
    "    fine_samples_rgb = torch.zeros((nb_coarse_samples + nb_fine_samples,3))\n",
    "\n",
    "    for i in range(nb_fine_samples):\n",
    "        fine_samples_density[i],fine_samples_rgb[i] = query_from_NeRF(fine_net,origin +  (direction * fine_samples_distance[i]),direction)\n",
    "\n",
    "    fine_colour_of_ray = torch.zeros(3)\n",
    "    for i in range(nb_coarse_samples+nb_fine_samples-1):\n",
    "        Ti = torch.tensor(0)\n",
    "        for k in range(i-1):\n",
    "            #TODO : this should be simplified with exponent rules to not have to do a loop\n",
    "            Ti = torch.add(Ti, fine_samples_density[k]* (fine_samples_distance[k+1] - fine_samples_distance[k]))\n",
    "        Ti = torch.exp(-Ti)\n",
    "\n",
    "        fine_colour_of_ray =torch.add(fine_colour_of_ray, Ti * (1 - torch.exp(-fine_samples_density[i] * (fine_samples_distance[i+1] - fine_samples_distance[i]))))\n",
    "\n",
    "    return coarse_colour_of_ray,fine_colour_of_ray\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to synthesize a view from a focal distance and a pose (direction and position)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_view(coarse_net, fine_net,focal, pose,H,W):\n",
    "\n",
    "    rays_origin, rays_dir = get_all_rays(H,W,focal,pose)\n",
    "    pixels = np.zeros((H,W,3))\n",
    "    for i in range(H):\n",
    "        for j in range(W):\n",
    "            \n",
    "            colour = hierarchical_volume_sampling(64,128,coarse_net,fine_net,rays_origin[i+j],rays_dir[i+j])\n",
    "            pixels[i,j] = colour\n",
    "\n",
    "    return pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training step function.\n",
    "\n",
    "Training data should be in the format [imgs, poses, render_poses, [H, W, focal], data_split_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_step(optimizer,criterion,coarse_net,fine_net,training_data):\n",
    "    \n",
    "    #randomly select a training img\n",
    "\n",
    "    image_nb = np.random.choice(training_data[4][0])\n",
    "\n",
    "    true_pixels = training_data[0][image_nb]\n",
    "    pose = training_data[1][image_nb]\n",
    "    H,W,focal = training_data[3]\n",
    "    #randomly select a batch of rays\n",
    "    \n",
    "    max_batch_size = 100\n",
    "    batch_size = max_batch_size\n",
    "    if H < batch_size:\n",
    "        batch_size = H\n",
    "    if W < batch_size:\n",
    "        batch_size = W\n",
    "\n",
    "    x_pixels_coord = np.random.choice(np.arange(W),batch_size,replace=False).reshape((batch_size,1))\n",
    "    y_pixels_coord = np.random.choice(np.arange(H),batch_size,replace=False).reshape((batch_size,1))\n",
    "\n",
    "    rays_o,rays_d = get_random_ray_batch(H,W,focal,pose,x_pixels_coord,y_pixels_coord)\n",
    "\n",
    "    fine_col = torch.zeros(batch_size,3)\n",
    "    coarse_col = torch.zeros(batch_size,3)\n",
    "\n",
    "    for i in range(batch_size):\n",
    "        a = hierarchical_volume_sampling(64,128,coarse_net,fine_net,rays_o[i], rays_d[i])\n",
    "        coarse_col[i], fine_col[i] = a\n",
    "\n",
    "    #compute loss and back prop\n",
    "    optimizer.zero_grad()    \n",
    "    loss = criterion(coarse_col,fine_col,true_pixels[x_pixels_coord,y_pixels_coord])\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to decrease learning rate as specified in paper: \"learning rate that begins at 5 ×10−4 and decays exponentially to 5 ×10−5 over the course of optimization\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_lr_decay_factor(total_steps,initial_val,end_val):\n",
    "    # compute the decay from the inverse of y = a*decayfactor ^ steps\n",
    "\n",
    "    diff = abs(initial_val - end_val)\n",
    "    a = torch.tensor(diff/initial_val)\n",
    "    b = torch.tensor(total_steps)\n",
    "    decay_factor = torch.log(a) / torch.log(b) # base change rule\n",
    "    return decay_factor\n",
    "\n",
    "def update_lr(optimizer,decay,initial_lr,current_step):\n",
    "    new_lr = initial_lr * decay**current_step\n",
    "\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = new_lr\n",
    "\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MAIN LOOP with data loading,instantiation of NeRF networks and training of the NeRFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load data. Load in halfres to get better performances for testing\n",
    "synthetic_data = load_blender_data('data/nerf_synthetic/lego',half_res=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_19643/4072648898.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  encoded[i] = torch.sin(torch.tensor(np.power(2,i)*np.pi*value))\n",
      "/tmp/ipykernel_19643/4072648898.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  encoded[i+1] = torch.cos(torch.tensor(np.power(2,i)*np.pi*value))\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#Instantiate NeRF networks\n",
    "fine_scene1 = NeRF()\n",
    "coarse_scene1 = NeRF()\n",
    "\n",
    "# attribute loss fct\n",
    "criterion = loss_fct\n",
    "\n",
    "# set optimizer\n",
    "initial_lr = 5e-04\n",
    "optimizer = torch.optim.Adam(list(coarse_scene1.parameters()) + list(fine_scene1.parameters()), lr=initial_lr,eps=1e-08) \n",
    "\n",
    "#set nb of training steps and lr_decay\n",
    "nb_steps = 1000\n",
    "lr_decay = compute_lr_decay_factor(nb_steps,initial_lr,5e-05)\n",
    "\n",
    "# set array to plot losses\n",
    "losses = np.zeros(nb_steps)\n",
    "# start training\n",
    "for step in range(nb_steps):\n",
    "    print(step,end='\\r')\n",
    "    losses[step] = training_step(optimizer,criterion,coarse_scene1,fine_scene1,synthetic_data)\n",
    "    update_lr(optimizer,lr_decay,initial_lr,step)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show final trained result from a novel view"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3656675e5c9ddbad44bbaefbc4c978fb0abed373f282a0307983d4ade1822146"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('iml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
