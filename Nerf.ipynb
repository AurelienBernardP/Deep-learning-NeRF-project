{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports needed to load the data, train the model, and plot its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data to train the network: Code was adapted from the official NeRF repository to work with PyTorch. https://github.com/bmild/nerf/blob/master/load_blender.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Translation matrix but 't' is only considered to be on the z axis. It basically translates the given point by t in the z axis direction.\n",
    "# Note : This is weird as in spherical coordinates for which these matrices are used, the given 't' value is usually the radius.\n",
    "trans_t = lambda t : torch.tensor([\n",
    "    [1,0,0,0],\n",
    "    [0,1,0,0],\n",
    "    [0,0,1,t],\n",
    "    [0,0,0,1],\n",
    "], dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Rx rotation matrix rotation. Rotation around the x axis. Angle given by 'phi'\n",
    "rot_phi = lambda phi : torch.tensor([\n",
    "    [1,0,0,0],\n",
    "    [0,torch.cos(phi),-torch.sin(phi),0],\n",
    "    [0,torch.sin(phi), torch.cos(phi),0],\n",
    "    [0,0,0,1],\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Rz rotation matrix. Rotation around the z axis. Angle given by 'th'\n",
    "rot_theta = lambda th : torch.tensor([\n",
    "    [torch.cos(th),0,-torch.sin(th),0],\n",
    "    [0,1,0,0],\n",
    "    [torch.sin(th),0, torch.cos(th),0],\n",
    "    [0,0,0,1],\n",
    "], dtype=torch.float32)\n",
    "\n",
    "\n",
    "def pose_spherical(theta, phi, radius):\n",
    "    '''\n",
    "        not sure about this :- seems to perform some sort of position transformation from a 'camera' reference frame to a general 'world' reference frame in carthesian cooridnates\n",
    "    '''\n",
    "    # translate point along the z axis by 'radius'\n",
    "    c2w = trans_t(torch.tensor(radius))\n",
    "\n",
    "    # rotate point by phi around x axis\n",
    "    c2w = torch.matmul(rot_phi(torch.tensor(phi/180.*np.pi)) , c2w)\n",
    "\n",
    "    # rotate point by theta around z(?) axis\n",
    "    c2w = torch.matmul(rot_theta(torch.tensor((theta/180.*np.pi))) , c2w)\n",
    "\n",
    "    # I don't understand this transform. It looks like its scaling the x coordinates by -1\n",
    "    c2w = torch.matmul(torch.tensor(np.array([[-1,0,0,0],\n",
    "                                              [ 0,0,1,0],\n",
    "                                              [ 0,1,0,0],\n",
    "                                              [ 0,0,0,1]])).double() , c2w.double()) # Had to call double() on both tensors or the matmul() wouldn't work for some reason\n",
    "    return c2w\n",
    "    \n",
    "\n",
    "\n",
    "def load_blender_data(basedir, half_res=False, testskip=1):\n",
    "    '''\n",
    "        inputs : \n",
    "                basedir : (str) containing the base directory where the data can be found\n",
    "                half_res: (bool) reduces the resolution of the images to haf of its pixels if true\n",
    "                testskip: (int) sep with which images are loaded: 1 => all loaded, 2 => only half of them ...\n",
    "        outputs:\n",
    "                imgs: numpy array of images as array of RGB values por each pixel \n",
    "                not sure about this :- poses: numpy array of 4x4 transformation matrices giving the position and angle of the object with respect to a general reference frame : more info on how they work on https://www.brainvoyager.com/bv/doc/UsersGuide/CoordsAndTransforms/SpatialTransformationMatrices.html\n",
    "                render_poses: numpy array of 4x4 transformation matrices giving the position of the camera relative to a general reference frame in a circular path for rendering purposes.\n",
    "                [H, W, focal] : height of screen on which object is projected (pixels), width of screen on which object is proejected (pixels), focal distance : distance between camera and center of screen in some arbitrary unit\n",
    "                i_split :array of 3 arrays with the numbers of the indices of [ train , val, test] images\n",
    "    '''\n",
    "    splits = ['train', 'val', 'test']\n",
    "    metas = {}\n",
    "    for s in splits:\n",
    "        with open(os.path.join(basedir, 'transforms_{}.json'.format(s)), 'r') as fp:\n",
    "            metas[s] = json.load(fp)\n",
    "\n",
    "    all_imgs = []\n",
    "    all_poses = []\n",
    "    counts = [0]\n",
    "    for s in splits:\n",
    "        meta = metas[s]\n",
    "        imgs = []\n",
    "        poses = []\n",
    "        if s=='train' or testskip==0:\n",
    "            skip = 1\n",
    "        else:\n",
    "            skip = testskip\n",
    "            \n",
    "        for frame in meta['frames'][::skip]:\n",
    "            fname = os.path.join(basedir, frame['file_path'] + '.png')\n",
    "            imgs.append(torchvision.io.read_image(fname)) # Load images as RGB torch tensors\n",
    "            poses.append(np.array(frame['transform_matrix']))\n",
    "        poses = np.array(poses).astype(np.float32)\n",
    "        counts.append(counts[-1] + len(imgs))\n",
    "        all_imgs.append(imgs)\n",
    "        all_poses.append(poses)\n",
    "    \n",
    "    i_split = [np.arange(counts[i], counts[i+1]) for i in range(3)]\n",
    "    imgs = np.concatenate(all_imgs, 0)\n",
    "    poses = np.concatenate(all_poses, 0)\n",
    "    \n",
    "    H, W = imgs[0].shape[:2]\n",
    "    camera_angle_x = float(meta['camera_angle_x']) # FOV angle of camera\n",
    "    focal = .5 * W / np.tan(.5 * camera_angle_x) # focal distance of camera to the screen.\n",
    "    \n",
    "    render_poses = torch.stack([pose_spherical(angle, -30.0, 4.0) for angle in np.linspace(-180,180,40+1)[:-1]],0)\n",
    "\n",
    "    if half_res:\n",
    "        for i,img in enumerate(imgs):\n",
    "            imgs[i] = torchvision.transforms.functional.resize(img, [400, 400]).numpy()\n",
    "        H = H//2\n",
    "        W = W//2\n",
    "        focal = focal/2.\n",
    "        \n",
    "    return imgs, poses, render_poses, [H, W, focal], i_split \n",
    "\n",
    "synthetic_data = load_blender_data('data/nerf_synthetic/lego',half_res=True) # load in halfres to get better performances at dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that will return the ray direction and origin for the images given in the dataset. Code adapted from the original NeRF repository : https://github.com/bmild/nerf/blob/master/run_nerf_helpers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10]],\n",
       " \n",
       "         [[10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10]],\n",
       " \n",
       "         [[10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10]],\n",
       " \n",
       "         [[10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10]],\n",
       " \n",
       "         [[10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10]],\n",
       " \n",
       "         [[10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10]],\n",
       " \n",
       "         [[10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10]],\n",
       " \n",
       "         [[10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10]],\n",
       " \n",
       "         [[10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10]],\n",
       " \n",
       "         [[10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10],\n",
       "          [10, 10, 10]]]),\n",
       " tensor([[[ -10.,  -10.,  -10.],\n",
       "          [   0.,    0.,    0.],\n",
       "          [  10.,   10.,   10.],\n",
       "          [  20.,   20.,   20.],\n",
       "          [  30.,   30.,   30.],\n",
       "          [  40.,   40.,   40.],\n",
       "          [  50.,   50.,   50.],\n",
       "          [  60.,   60.,   60.],\n",
       "          [  70.,   70.,   70.],\n",
       "          [  80.,   80.,   80.]],\n",
       " \n",
       "         [[ -20.,  -20.,  -20.],\n",
       "          [ -10.,  -10.,  -10.],\n",
       "          [   0.,    0.,    0.],\n",
       "          [  10.,   10.,   10.],\n",
       "          [  20.,   20.,   20.],\n",
       "          [  30.,   30.,   30.],\n",
       "          [  40.,   40.,   40.],\n",
       "          [  50.,   50.,   50.],\n",
       "          [  60.,   60.,   60.],\n",
       "          [  70.,   70.,   70.]],\n",
       " \n",
       "         [[ -30.,  -30.,  -30.],\n",
       "          [ -20.,  -20.,  -20.],\n",
       "          [ -10.,  -10.,  -10.],\n",
       "          [   0.,    0.,    0.],\n",
       "          [  10.,   10.,   10.],\n",
       "          [  20.,   20.,   20.],\n",
       "          [  30.,   30.,   30.],\n",
       "          [  40.,   40.,   40.],\n",
       "          [  50.,   50.,   50.],\n",
       "          [  60.,   60.,   60.]],\n",
       " \n",
       "         [[ -40.,  -40.,  -40.],\n",
       "          [ -30.,  -30.,  -30.],\n",
       "          [ -20.,  -20.,  -20.],\n",
       "          [ -10.,  -10.,  -10.],\n",
       "          [   0.,    0.,    0.],\n",
       "          [  10.,   10.,   10.],\n",
       "          [  20.,   20.,   20.],\n",
       "          [  30.,   30.,   30.],\n",
       "          [  40.,   40.,   40.],\n",
       "          [  50.,   50.,   50.]],\n",
       " \n",
       "         [[ -50.,  -50.,  -50.],\n",
       "          [ -40.,  -40.,  -40.],\n",
       "          [ -30.,  -30.,  -30.],\n",
       "          [ -20.,  -20.,  -20.],\n",
       "          [ -10.,  -10.,  -10.],\n",
       "          [   0.,    0.,    0.],\n",
       "          [  10.,   10.,   10.],\n",
       "          [  20.,   20.,   20.],\n",
       "          [  30.,   30.,   30.],\n",
       "          [  40.,   40.,   40.]],\n",
       " \n",
       "         [[ -60.,  -60.,  -60.],\n",
       "          [ -50.,  -50.,  -50.],\n",
       "          [ -40.,  -40.,  -40.],\n",
       "          [ -30.,  -30.,  -30.],\n",
       "          [ -20.,  -20.,  -20.],\n",
       "          [ -10.,  -10.,  -10.],\n",
       "          [   0.,    0.,    0.],\n",
       "          [  10.,   10.,   10.],\n",
       "          [  20.,   20.,   20.],\n",
       "          [  30.,   30.,   30.]],\n",
       " \n",
       "         [[ -70.,  -70.,  -70.],\n",
       "          [ -60.,  -60.,  -60.],\n",
       "          [ -50.,  -50.,  -50.],\n",
       "          [ -40.,  -40.,  -40.],\n",
       "          [ -30.,  -30.,  -30.],\n",
       "          [ -20.,  -20.,  -20.],\n",
       "          [ -10.,  -10.,  -10.],\n",
       "          [   0.,    0.,    0.],\n",
       "          [  10.,   10.,   10.],\n",
       "          [  20.,   20.,   20.]],\n",
       " \n",
       "         [[ -80.,  -80.,  -80.],\n",
       "          [ -70.,  -70.,  -70.],\n",
       "          [ -60.,  -60.,  -60.],\n",
       "          [ -50.,  -50.,  -50.],\n",
       "          [ -40.,  -40.,  -40.],\n",
       "          [ -30.,  -30.,  -30.],\n",
       "          [ -20.,  -20.,  -20.],\n",
       "          [ -10.,  -10.,  -10.],\n",
       "          [   0.,    0.,    0.],\n",
       "          [  10.,   10.,   10.]],\n",
       " \n",
       "         [[ -90.,  -90.,  -90.],\n",
       "          [ -80.,  -80.,  -80.],\n",
       "          [ -70.,  -70.,  -70.],\n",
       "          [ -60.,  -60.,  -60.],\n",
       "          [ -50.,  -50.,  -50.],\n",
       "          [ -40.,  -40.,  -40.],\n",
       "          [ -30.,  -30.,  -30.],\n",
       "          [ -20.,  -20.,  -20.],\n",
       "          [ -10.,  -10.,  -10.],\n",
       "          [   0.,    0.,    0.]],\n",
       " \n",
       "         [[-100., -100., -100.],\n",
       "          [ -90.,  -90.,  -90.],\n",
       "          [ -80.,  -80.,  -80.],\n",
       "          [ -70.,  -70.,  -70.],\n",
       "          [ -60.,  -60.,  -60.],\n",
       "          [ -50.,  -50.,  -50.],\n",
       "          [ -40.,  -40.,  -40.],\n",
       "          [ -30.,  -30.,  -30.],\n",
       "          [ -20.,  -20.,  -20.],\n",
       "          [ -10.,  -10.,  -10.]]]))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_all_rays(H, W, focal, pose):\n",
    "    \"\"\"Get ray origins, and directions from a pinhole camera. given the 'pose' transform matrix to transform the direction and position \n",
    "       from standard camera at origin to actual position and direction in world cooridnates\"\"\"\n",
    "    i, j = torch.meshgrid(torch.linspace(0, W-1, W), torch.linspace(0, H-1, H), indexing='xy')\n",
    "    dirs = torch.stack([(i-W*.5)/focal, -(j-H*.5)/focal, -torch.ones_like(i)], -1)\n",
    "    rays_d =  torch.sum((dirs[..., np.newaxis, :]) * pose[:3,:3], -1)\n",
    "    rays_o = pose[:3,-1].expand(rays_d.shape)\n",
    "    return rays_o, rays_d\n",
    "\n",
    "def get_random_ray_batch(H,W, focal, pose, x_pixel_coord,y_pixel_coord):\n",
    "   \"get randomly nb_rays from the rays tha go through each pixel.\"\n",
    "   rays_o,rays_d = get_all_rays(H,W,focal,pose)\n",
    "\n",
    "   return rays_o[x_pixel_coord,y_pixel_coord], rays_d[x_pixel_coord,y_pixel_coord]\n",
    "\n",
    "get_all_rays(10,10,1,torch.tensor([[10,10,10,10],[10,10,10,10],[10,10,10,10],[10,10,10,10]]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the NeRF neural network as defined in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeRF(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        input_position = 60\n",
    "        input_direction = 24\n",
    "        output_density = 1\n",
    "        output_colour = 3\n",
    "        hidden_features = 256\n",
    "\n",
    "        self.l1 = nn.Linear(input_position,  hidden_features)\n",
    "        self.l2 = nn.Linear(hidden_features, hidden_features)\n",
    "        self.l3 = nn.Linear(hidden_features, hidden_features)\n",
    "        self.l4 = nn.Linear(hidden_features, hidden_features)\n",
    "        self.l5 = nn.Linear(hidden_features + input_position, hidden_features)\n",
    "        self.l6 = nn.Linear(hidden_features, hidden_features)\n",
    "        self.l7 = nn.Linear(hidden_features, hidden_features)\n",
    "        self.l8 = nn.Linear(hidden_features, hidden_features)        \n",
    "        self.l9 = nn.Linear(hidden_features+input_direction, hidden_features+output_density)\n",
    "        self.l10 = nn.Linear(hidden_features, 128)\n",
    "        self.l11 = nn.Linear(128, output_colour)\n",
    "\n",
    "        self.activationReLU = nn.ReLU()\n",
    "        self.activationSigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, pos, dir):\n",
    "\n",
    "        h1 = self.activationReLU(self.l1(pos))\n",
    "        h2 = self.activationReLU(self.l2(h1))\n",
    "        h3 = self.activationReLU(self.l3(h2))\n",
    "        h4 = self.activationReLU(self.l4(h3))\n",
    "        h5 = self.activationReLU(self.l5(torch.cat([h4, pos]))) \n",
    "        h6 = self.activationReLU(self.l6(h5))\n",
    "        h7 = self.activationReLU(self.l7(h6))\n",
    "        h8 = self.l8(h7) # no activation function before layer 9\n",
    "        partial_h9 = self.l9(h8)\n",
    "        density = partial_h9[:,0]\n",
    "        h9 = self.activationReLU(torch.cat([partial_h9[:,1:] + dir])) #### cat sur la bonne dimension\n",
    "        h10 = self.activationReLU(self.l10(h9))\n",
    "        colour = self.activationReLU(self.l11(h10))\n",
    "\n",
    "        return density, colour\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a coarse and fine network to start the scene function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_scene1 = NeRF()\n",
    "coarse_scene1 = NeRF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function and optimizer\n",
    "\n",
    "TODO: make it support tensors ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fct(rgb_pred_coarse,rgb_pred_fine,rgb_true):\n",
    "    loss = 0\n",
    "    for i in range(len(rgb_pred_coarse)):\n",
    "        loss += (torch.norm(torch.sub(rgb_pred_coarse[i], rgb_true[i]),2) + torch.norm(torch.sub(rgb_pred_fine[i], rgb_true[i]),2))\n",
    "\n",
    "criterion = loss_fct\n",
    "\n",
    "optimizer = torch.optim.Adam(list(coarse_scene1.parameters()) + list(fine_scene1.parameters()), lr=5e-04,eps=1e-08) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the encoding function that will take the inputs of the neural network and project them to a higher dimension input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_fct(value,level):\n",
    "\n",
    "    encoded = torch.zeros(level*2)\n",
    "\n",
    "    for i in range(0,level*2, 2):\n",
    "        encoded[i] = torch.sin(torch.pow(torch.Tensor([2]),i)*torch.pi*value)\n",
    "        encoded[i+1] = torch.cos(torch.pow(torch.Tensor([2]),i)*torch.pi*value)\n",
    "        \n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Query fct to get a sample from NeRF networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_from_NeRF(network,pos,direction):\n",
    "\n",
    "    l_pos = 10\n",
    "    l_dir = 4\n",
    "\n",
    "    pos_query = torch.empty(0)\n",
    "    dir_query = torch.empty(0)\n",
    "\n",
    "    for coord in pos:\n",
    "        pos_query = torch.cat((pos_query, encoding_fct(coord, l_pos)))\n",
    "    \n",
    "    for d in direction:\n",
    "        dir_query = torch.cat((dir_query, encoding_fct(d,l_dir)))\n",
    "\n",
    "    density, colour = network.forward(pos_query,dir_query)\n",
    "    return density , colour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO : make fct that will return pdf function from weights vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pdf_sampling(weights,min_dist,distance_step,nb_samples):\n",
    "\n",
    "    cumulative_probability = torch.zeros(weights.size() + 2)\n",
    "    bound_distance = torch.zeros(weights.size() + 2)\n",
    "    bound_distance[0] = min_dist\n",
    "\n",
    "    for i in range(1,cumulative_probability.size()):\n",
    "        cumulative_probability[i] = cumulative_probability[i-1] + weights[i-1]\n",
    "        bound_distance[i] = bound_distance[i-1] + distance_step\n",
    "\n",
    "    samples = torch.zeros(nb_samples)\n",
    "\n",
    "    #generate as many numbers in the [0,1[ range as there are samples\n",
    "    random_samples = torch.rand(nb_samples)\n",
    "\n",
    "    for i, rand_nb in enumerate(random_samples):\n",
    "        # for each random sample check within each bin of the cumulative pdf it is \n",
    "        for j in range(cumulative_probability.size() - 1 ):\n",
    "            \n",
    "            if(rand_nb < cumulative_probability[j+1] and rand_nb >= cumulative_probability[j]):\n",
    "                # when in appropriate bin, redraw uniformly between lowest and largest distance of the bin\n",
    "                samples[i] = (bound_distance[j] - bound_distance[j+1]) * torch.rand(1) + bound_distance[j+1]\n",
    "                break\n",
    "\n",
    "\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hierarchical volume sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hierarchical_volume_sampling(nb_coarse_samples,nb_fine_samples,coarse_net,fine_net,origin, direction,min_dist=0.,max_dist = 1.):\n",
    "    \n",
    "    coarse_samples = torch.zeros(nb_coarse_samples)\n",
    "    weights = torch.zeros(nb_coarse_samples)\n",
    "    weights = torch.zeros(nb_coarse_samples)\n",
    "    distance_fraction = (min_dist + max_dist) / (nb_coarse_samples + 1)\n",
    "    \n",
    "    coarse_samples_distance = np.zeros(nb_coarse_samples)\n",
    "    coarse_colour_of_ray = torch.zeros(3)\n",
    "\n",
    "    ##TODO : coarse colour sampling\n",
    "\n",
    "    for i in range(nb_coarse_samples):\n",
    "        \n",
    "        # equidistant samples with sam direction\n",
    "        coarse_samples_distance[i] = (min_dist + (distance_fraction * (i + 1))) #sorted by definition\n",
    "\n",
    "        # we only care for the density of the coarse samples\n",
    "        coarse_samples[i] = (query_from_NeRF(coarse_net, origin +  (direction * coarse_samples_distance[i]),direction))[0]\n",
    "\n",
    "        #compute Ti value\n",
    "        Ti = 0\n",
    "        for k in range(i):\n",
    "            #TODO : this should be simplified with exponent rules to not have to do a loop\n",
    "            Ti += coarse_samples[k]*distance_fraction\n",
    "        Ti = np.exp(-Ti)\n",
    "\n",
    "        #compute weights of importance of sample to generate pdf later\n",
    "        weights[i] = Ti * (1 - np.exp(-distance_fraction * coarse_samples[i]))\n",
    "    \n",
    "    # normalize\n",
    "    total_weights = torch.sum(weights)\n",
    "    weights = torch.div(weights,total_weights)\n",
    "    \n",
    "    # generate sample locations from pdf made with the weights:\n",
    "    fine_samples_distance = pdf_sampling(weights,min_dist,distance_fraction)\n",
    "\n",
    "    #add the coarse locations to the set of lecations and sort them in incresing order\n",
    "    fine_samples_distance = torch.cat((fine_samples_distance,coarse_samples_distance))\n",
    "    fine_samples_distance,_ = torch.sort(fine_samples_distance)\n",
    "    \n",
    "    fine_samples_density = torch.zeros((nb_coarse_samples + nb_fine_samples))\n",
    "    fine_samples_rgb = torch.zeros((nb_coarse_samples + nb_fine_samples,3))\n",
    "\n",
    "    for i in range(nb_fine_samples):\n",
    "        fine_samples_density[i],fine_samples_rgb[i] = query_from_NeRF(fine_net,origin +  (direction * fine_samples_distance[i]),direction)\n",
    "\n",
    "    fine_colour_of_ray = torch.zeros(3)\n",
    "    for i in range(fine_samples_density.size()-1):\n",
    "        Ti = 0\n",
    "        for k in range(i-1):\n",
    "            #TODO : this should be simplified with exponent rules to not have to do a loop\n",
    "            Ti += fine_samples_density[k]* (fine_samples_distance[k+1] - fine_samples_distance[k])\n",
    "        Ti = np.exp(-Ti)\n",
    "\n",
    "        fine_colour_of_ray += Ti * (1 - torch.exp(-fine_samples_density[i] * (fine_samples_distance[i+1] - fine_samples_distance[i])))\n",
    "\n",
    "    return fine_colour_of_ray\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function to synthesize a view from a focal distance and a pose (direction and position)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synthesize_view(coarse_net, fine_net,focal, pose,H,W):\n",
    "\n",
    "    rays_origin, rays_dir = get_all_rays(H,W,focal,pose)\n",
    "    pixels = np.zeros((H,W,3))\n",
    "    for i in range(H):\n",
    "        for j in range(W):\n",
    "            \n",
    "            colour = hierarchical_volume_sampling(64,128,coarse_net,fine_net,rays_origin[i+j],rays_dir[i+j])\n",
    "            pixels[i,j] = colour\n",
    "\n",
    "    return pixels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training step function. Reduce the training rate at each step.\n",
    "\n",
    "Training data should be in the format [imgs, poses, render_poses, [H, W, focal], i_train_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2]\n",
      " [0]]\n",
      "[[1]\n",
      " [0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[8],\n",
       "       [1]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def training_step(coarse_net= None,fine_net= None,training_data= None, img = None):\n",
    "    \n",
    "    image_nb = np.random.choice(training_data[4],1)\n",
    "    true_pixels = training_data[0,image_nb]\n",
    "    pose = training_data[1,image_nb]\n",
    "    H,W,focal = training_data[3,image_nb]\n",
    "    \n",
    "    \n",
    "    #randomly select a batch of rays\n",
    "    batch_size = 2\n",
    "    x_pixels_coord = np.random.choice(np.arange(3),batch_size,replace=False).reshape((batch_size,1))\n",
    "    y_pixels_coord = np.random.choice(np.arange(3),batch_size,replace=False).reshape((batch_size,1))\n",
    "    \n",
    "    rays_o,rays_d = get_random_ray_batch(H,W,focal,pose,x_pixels_coord,y_pixels_coord)\n",
    "    \n",
    "    loss = 0\n",
    "    for i in range(batch_size):\n",
    "        loss += criterion()\n",
    "    return img[x_pixels_coord,y_pixels_coord]\n",
    "\n",
    "training_step(img = np.array([[1,2,3],[4,5,6],[7,8,9]]))\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3656675e5c9ddbad44bbaefbc4c978fb0abed373f282a0307983d4ade1822146"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('iml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
