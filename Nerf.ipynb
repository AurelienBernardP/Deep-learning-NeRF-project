{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports needed to load the data, train the model, and plot its performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data as data\n",
    "\n",
    "from PIL import Image\n",
    "import torchvision\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load data to train the network: Code was adapted from the official NeRF repository to work with PyTorch. https://github.com/bmild/nerf/blob/master/load_blender.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<__array_function__ internals>:5: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.\n",
      "<__array_function__ internals>:5: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n"
     ]
    }
   ],
   "source": [
    "# Translation matrix but 't' is only considered to be on the z axis. It basically translates the given point by t in the z axis direction.\n",
    "# Note : This is weird as in spherical coordinates for which these matrices are used, the given 't' value is usually the radius.\n",
    "trans_t = lambda t : torch.tensor([\n",
    "    [1,0,0,0],\n",
    "    [0,1,0,0],\n",
    "    [0,0,1,t],\n",
    "    [0,0,0,1],\n",
    "], dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Rx rotation matrix rotation. Rotation around the x axis. Angle given by 'phi'\n",
    "rot_phi = lambda phi : torch.tensor([\n",
    "    [1,0,0,0],\n",
    "    [0,torch.cos(phi),-torch.sin(phi),0],\n",
    "    [0,torch.sin(phi), torch.cos(phi),0],\n",
    "    [0,0,0,1],\n",
    "], dtype=torch.float32)\n",
    "\n",
    "# Rz rotation matrix. Rotation around the z axis. Angle given by 'th'\n",
    "rot_theta = lambda th : torch.tensor([\n",
    "    [torch.cos(th),0,-torch.sin(th),0],\n",
    "    [0,1,0,0],\n",
    "    [torch.sin(th),0, torch.cos(th),0],\n",
    "    [0,0,0,1],\n",
    "], dtype=torch.float32)\n",
    "\n",
    "\n",
    "def pose_spherical(theta, phi, radius):\n",
    "    '''\n",
    "        not sure about this :- seems to perform some sort of position transformation from a 'camera' reference frame to a general 'world' reference frame in carthesian cooridnates\n",
    "    '''\n",
    "    # translate point along the z axis by 'radius'\n",
    "    c2w = trans_t(torch.tensor(radius))\n",
    "\n",
    "    # rotate point by phi around x axis\n",
    "    c2w = torch.matmul(rot_phi(torch.tensor(phi/180.*np.pi)) , c2w)\n",
    "\n",
    "    # rotate point by theta around z(?) axis\n",
    "    c2w = torch.matmul(rot_theta(torch.tensor((theta/180.*np.pi))) , c2w)\n",
    "\n",
    "    # I don't understand this transform. It looks like its scaling the x coordinates by -1\n",
    "    c2w = torch.matmul(torch.tensor(np.array([[-1,0,0,0],\n",
    "                                              [ 0,0,1,0],\n",
    "                                              [ 0,1,0,0],\n",
    "                                              [ 0,0,0,1]])).double() , c2w.double()) # Had to call double() on both tensors or the matmul() wouldn't work for some reason\n",
    "    return c2w\n",
    "    \n",
    "\n",
    "\n",
    "def load_blender_data(basedir, half_res=False, testskip=1):\n",
    "    '''\n",
    "        inputs : \n",
    "                basedir : (str) containing the base directory where the data can be found\n",
    "                half_res: (bool) reduces the resolution of the images to haf of its pixels if true\n",
    "                testskip: (int) sep with which images are loaded: 1 => all loaded, 2 => only half of them ...\n",
    "        outputs:\n",
    "                imgs: numpy array of images as array of RGB values por each pixel \n",
    "                not sure about this :- poses: numpy array of 4x4 transformation matrices giving the position and angle of the object with respect to a general reference frame : more info on how they work on https://www.brainvoyager.com/bv/doc/UsersGuide/CoordsAndTransforms/SpatialTransformationMatrices.html\n",
    "                render_poses: numpy array of 4x4 transformation matrices giving the position of the camera relative to a general reference frame in a circular path for rendering purposes.\n",
    "                [H, W, focal] : height of screen on which object is projected (pixels), width of screen on which object is proejected (pixels), focal distance : distance between camera and center of screen in some arbitrary unit\n",
    "                i_split :array of 3 arrays with the numbers of the indices of [ train , val, test] images\n",
    "    '''\n",
    "    splits = ['train', 'val', 'test']\n",
    "    metas = {}\n",
    "    for s in splits:\n",
    "        with open(os.path.join(basedir, 'transforms_{}.json'.format(s)), 'r') as fp:\n",
    "            metas[s] = json.load(fp)\n",
    "\n",
    "    all_imgs = []\n",
    "    all_poses = []\n",
    "    counts = [0]\n",
    "    for s in splits:\n",
    "        meta = metas[s]\n",
    "        imgs = []\n",
    "        poses = []\n",
    "        if s=='train' or testskip==0:\n",
    "            skip = 1\n",
    "        else:\n",
    "            skip = testskip\n",
    "            \n",
    "        for frame in meta['frames'][::skip]:\n",
    "            fname = os.path.join(basedir, frame['file_path'] + '.png')\n",
    "            imgs.append(torchvision.io.read_image(fname)) # Load images as RGB torch tensors\n",
    "            poses.append(np.array(frame['transform_matrix']))\n",
    "        poses = np.array(poses).astype(np.float32)\n",
    "        counts.append(counts[-1] + len(imgs))\n",
    "        all_imgs.append(imgs)\n",
    "        all_poses.append(poses)\n",
    "    \n",
    "    i_split = [np.arange(counts[i], counts[i+1]) for i in range(3)]\n",
    "    imgs = np.concatenate(all_imgs, 0)\n",
    "    poses = np.concatenate(all_poses, 0)\n",
    "    \n",
    "    H, W = imgs[0].shape[:2]\n",
    "    camera_angle_x = float(meta['camera_angle_x']) # FOV angle of camera\n",
    "    focal = .5 * W / np.tan(.5 * camera_angle_x) # focal distance of camera to the screen.\n",
    "    \n",
    "    render_poses = torch.stack([pose_spherical(angle, -30.0, 4.0) for angle in np.linspace(-180,180,40+1)[:-1]],0)\n",
    "\n",
    "    if half_res:\n",
    "        for i,img in enumerate(imgs):\n",
    "            imgs[i] = torchvision.transforms.functional.resize(img, [400, 400]).numpy()\n",
    "        H = H//2\n",
    "        W = W//2\n",
    "        focal = focal/2.\n",
    "        \n",
    "    return imgs, poses, render_poses, [H, W, focal], i_split \n",
    "\n",
    "synthetic_data = load_blender_data('data/nerf_synthetic/lego',half_res=True) # load in halfres to get better performances at dev"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a function that will return the ray direction and origin for the images given in the dataset. Code adapted from the original NeRF repository : https://github.com/bmild/nerf/blob/master/run_nerf_helpers.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rays(H, W, focal, pose):\n",
    "    \"\"\"Get ray origins, and directions from a pinhole camera. given the 'pose' transform matrix to transform the direction and position \n",
    "       from standard camera at origin to actual position and direction in world cooridnates\"\"\"\n",
    "    i, j = torch.meshgrid(torch.range(W, dtype=torch.float32),\n",
    "                       torch.range(H, dtype=torch.float32), indexing='xy')\n",
    "    dirs = torch.stack([(i-W*.5)/focal, -(j-H*.5)/focal, -torch.ones_like(i)], -1)\n",
    "    rays_d = torch.sum(dirs[..., np.newaxis, :] * pose[:3, :3], -1)\n",
    "    rays_o = torch.broadcast_to(pose[:3, -1], torch.size(rays_d))\n",
    "    return rays_o, rays_d\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the NeRF neural network as defined in the paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeRF(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        input_position = 60\n",
    "        input_direction = 24\n",
    "        output_density = 1\n",
    "        output_colour = 3\n",
    "        hidden_features = 256\n",
    "\n",
    "        self.l1 = nn.Linear(input_position,  hidden_features)\n",
    "        self.l2 = nn.Linear(hidden_features, hidden_features)\n",
    "        self.l3 = nn.Linear(hidden_features, hidden_features)\n",
    "        self.l4 = nn.Linear(hidden_features, hidden_features)\n",
    "        self.l5 = nn.Linear(hidden_features + input_position, hidden_features)\n",
    "        self.l6 = nn.Linear(hidden_features, hidden_features)\n",
    "        self.l7 = nn.Linear(hidden_features, hidden_features)\n",
    "        self.l8 = nn.Linear(hidden_features, hidden_features)        \n",
    "        self.l9 = nn.Linear(hidden_features+input_direction, hidden_features+output_density)\n",
    "        self.l10 = nn.Linear(hidden_features, 128)\n",
    "        self.l11 = nn.Linear(128, output_colour)\n",
    "\n",
    "        self.activationReLU = nn.ReLU()\n",
    "        self.activationSigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, pos, dir):\n",
    "\n",
    "        h1 = self.activationReLU(self.l1(pos))\n",
    "        h2 = self.activationReLU(self.l2(h1))\n",
    "        h3 = self.activationReLU(self.l3(h2))\n",
    "        h4 = self.activationReLU(self.l4(h3))\n",
    "        h5 = self.activationReLU(self.l5(torch.cat([h4, pos]))) \n",
    "        h6 = self.activationReLU(self.l6(h5))\n",
    "        h7 = self.activationReLU(self.l7(h6))\n",
    "        h8 = self.l8(h7) # no activation function before layer 9\n",
    "        partial_h9 = self.l9(h8)\n",
    "        density = partial_h9[:,0]\n",
    "        h9 = self.activationReLU(torch.cat([partial_h9[:,1:] + dir])) #### cat sur la bonne dimension\n",
    "        h10 = self.activationReLU(self.l10(h9))\n",
    "        colour = self.activationReLU(self.l11(h10))\n",
    "\n",
    "        return density, colour\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instantiate a coarse and fine network to start the scene function approximation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fine_scene1 = NeRF()\n",
    "coarse_scene1 = NeRF()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the loss function and optimizer\n",
    "\n",
    "TODO: make it support tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_fct(rgb_pred_coarse,rgb_pred_fine,rgb_true):\n",
    "    loss = 0\n",
    "    for i in range(len(rgb_pred_coarse)):\n",
    "        loss += (torch.norm(torch.sub(rgb_pred_coarse[i], rgb_true[i]),2) + torch.norm(torch.sub(rgb_pred_fine[i], rgb_true[i]),2))\n",
    "\n",
    "criterion = loss_fct\n",
    "\n",
    "optimizer = torch.optim.Adam(list(coarse_scene1.parameters()) + list(fine_scene1.parameters()), lr=5e-04,eps=1e-08) \n",
    "### NOTE : do not forget to reduce the learning rate afterwards when doing the optimization steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the encoding function that will take the inputs of the neural network and project them to a higher dimension input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_fct(value,max_dim):\n",
    "    encoded = torch.zeros(max_dim*2)\n",
    "    for i in range(0,max_dim*2, 2):\n",
    "        encoded[i] = torch.sin(torch.pow(torch.Tensor([2]),i)*torch.pi*value)\n",
    "        encoded[i+1] = torch.cos(torch.pow(torch.Tensor([2]),i)*torch.pi*value)\n",
    "    return encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO : Define Hierarchical volume sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (3591139706.py, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipykernel_11877/3591139706.py\"\u001b[0;36m, line \u001b[0;32m9\u001b[0m\n\u001b[0;31m    return colour\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "def hierarchical_volume_sampling(nb_coarse_samples,nb_fine_samples,coarse_net,fine_net,min_dist,max_dist, origin,direction):\n",
    "    \n",
    "    for i in range(nb_coarse_samples):\n",
    "        # equidistant samples\n",
    "\n",
    "        for j in range(nb_fine_samples):\n",
    "            # samples randomly taken from a probability law made from the coarse samples weights\n",
    "\n",
    "    return colour"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3656675e5c9ddbad44bbaefbc4c978fb0abed373f282a0307983d4ade1822146"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('iml')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
